---
title: "ðŸ“– About"
---

The project covers the code and artifacts to help process Gitcoin Grants data from the [Allo Indexer Data API](https://indexer-production.fly.dev/data/). It is an instance of [Datadex](https://github.com/davidgasquez/datadex) allowing you and everyone else to:

- Add new data sources to the portal, collaborate on better models (ala Dune) or submit an interesting analysis.
- All in a permissionless way. Don't ask, fork it and improve the models, add a new source or update any script.
- Declarative stateless transformations tracked in git, executed in GitHub Actions and published to IPFS. Data, artifacts (like the entire DuckDB database), and models all version controlled.
- Share and explore dashboards and report with the world!

Read more about the [motivation and initial approach on my blog](https://davidgasquez.github.io/gitcoin-data/)!

## ðŸ“¦ Key Features

- **Open**.
  - Both code and data are fully open source.
  - Also relies on open standards/formats (Arrow ecosystem).
- **Permissionless**.
  - Clone and edit things away! You're not blocked by any API rate limits, or closed data like in Dune.
  - All other git features like branching, merging, pull requests, ... are available because all the data is transformed declaratively as code.
- **Decentralized**.
  - The project runs on a laptop, a server, a CI runner (that's the way is working right now) or a even decentralized compute network like [Bacalhau](https://www.bacalhau.org/). Oh, it even works in GitHub Codespaces so you don't even need to setup anything locally!
  - Data is stored in IPFS. You can run it locally, and it'll generate the same IPFS files if nothing has changed. The more people runst it, the more distributed the IPFS files will be!
  - Data comes from multiple sources and can be exposed in multiple ways.
- **Data as Code**.
  - Every commit generates all the table files and pushes them to IPFS. This means that we can always go back in time and see the data as it was at that point in time. For every commit, we'll have the data as it was at that point in time.
- **Modular**.
  - Each component can be replaced, extended, or removed. Works well in many environments (your laptop, in a cluster, or from the browser), and with multiple tools (tables are files at the end of the day).
- **Low Friction**.
  - Data (raw and processed) is already there! No need to write your own scripts. You can always reproduce it but getting started is as easy as pasting [a SQL query in your browser](https://shell.duckdb.org/) or doing `pd.read_parquet(url)` in a Notebook.
  - Every commit will also publish a set of Quarto Notebooks with the data. Could be used to generate reports/dahsboards, or as documentation.
- **Modern**
  - It supports all the cool things data engineers want; typing, tests, materialized views, dev branches, ...
  - Uses best practices (declarative transformations) and state of the art tooling (DuckDB).
